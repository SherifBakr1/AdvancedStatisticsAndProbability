---
title: "Problem Set 7, Spring 2024"
author: "Wendy Christensen"
output: pdf_document
---

```{r setup, include=TRUE}

# Load any packages, if any, that you use as part of your answers here
# For example: 

library(glmnet)
library(mlbench)
library(haven)
library(MASS)
library(survival)
library(survminer)

```


CONTEXT - HOUSE VALUES IN BOSTON, CIRCA 1970

This dataset was obtained through the mlbench package, which contains a subset of data sets available through the UCI Machine Learning Repository. From the help file:

Housing data for 506 census tracts of Boston from the 1970 census. The dataframe BostonHousing contains the original data by Harrison and Rubinfeld (1979).

The original data are 506 observations on 14 variables, medv being the target variable:

Continuous variables:

crim	    per capita crime rate by town 
zn       	proportion of residential land zoned for lots over 25,000 sq.ft  
indus   	proportion of non-retail business acres per town
nox	      nitric oxides concentration (parts per 10 million)
rm	      average number of rooms per dwelling
age	      proportion of owner-occupied units built prior to 1940
dis	      weighted distances to five Boston employment centres
rad	      index of accessibility to radial highways
tax	      full-value property-tax rate per USD 10,000
ptratio	  pupil-teacher ratio by town
b	        1000(B - 0.63)^2 where B is the proportion of blacks by town
lstat	    percentage of lower status of the population
medv	    median value of owner-occupied homes in USD 1000's

Categorical variables: 

chas	    Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)

## Question 1 - 10 points

First, load the data into memory. The variable types are already stored in this data set. 
```{r}

data(BostonHousing) # loads the BostonHousing dataset into memory from the mlbench package

str(BostonHousing)

```

Before you begin your analysis, you will split the data into a 70% training set and a 30% test set. First, save the number of rows in the data set for use in the splitting code.

```{r}

n <- nrow(BostonHousing) 

```

When splitting data into training/test data sets, it's good practice to set a random seed to create a split that's reproducible. For this question, use the following seed.

```{r}

set.seed(202211) 

```

In Problem Set 6, you were shown some code from the async to create a train-validate-test split

tvt2 <- sample(rep(0:2,c(round(n*.2),round(n*.2),n-2*round(n*.2))),n)

In this problem, however, you are splitting your data into just training and test sets (i.e., just two groups). You can make some changes to the rep() function contained in this line code to create a split for just train-test. To help you make these adaptations, the following code chunk contains the isolated version of what's contained in the tvt2 rep() function. Run it to see what it produces and then make alterations that will instead produce a set of 0's (test set, 30%) and 1's (training set, 70%) for splitting purposes.

```{r}

tvt2.rep <- rep(0:2,c(round(n*.2),round(n*.2),n-2*round(n*.2))) # The .2 in this function produces a 80% train/20% validation/20% test split in the data

tvt2.rep # Shows the result in the console window

table(tvt2.rep) # Shows a count of the 0's (test), 1's (valid), and 2's (train)

# Here is some room for you to change things and test how they work




```

Once you've found something that works, insert it into the blank space in tv.split to obtain a 70% training/30% test split. Display a table of the split to verify that approximately 70% of tv.split is equal to 1 and approximately 30% is equal to zero
```{r}

set.seed(202211) # Be sure to re-run this line right before running the following line

tv.split <- sample(rep(0:1, c(round(n * 0.3), n - round(n * 0.3))), n)

table(tv.split)   

dat.train <- BostonHousing[tv.split==1,] 
dat.test <- BostonHousing[tv.split==0,] 

```

## Question 2 - 10 points 

After completing Question 1, conduct a cross-validated ridge regression using the training data set. Use medv as the outcome and all of the other variables in the data set as the predictors. 

```{r}

# Your code to get the training data into the proper form to conduct cross-validated ridge regression

x_train <- model.matrix(medv ~ . - 1, data = dat.train)  # Remove the intercept
y_train <- dat.train$medv

# Your code to conduct cross-validated ridge regression

set.seed(202211)

cvfit.house.ridge <- cv.glmnet(x_train, y_train, alpha = 0, nfolds = 10)
print(cvfit.house.ridge)

```

For this question, the only lambda of interest is lambda.min. Make sure that lambda.min and the coefficients associated with it are visible in your knitted document. 

```{r}

# Display your lambda.min here

# Display the lambda.min
lambda_min <- cvfit.house.ridge$lambda.min
print(paste("lambda.min:", lambda_min))

# Display the coefficients associated with lambda.min
coefficients_min <- coef(cvfit.house.ridge, s = "lambda.min")
print("Coefficients associated with lambda.min:")
print(coefficients_min)



```

## Question 3 - 5 points

Using the results from Question 2, compute the mean squared prediction error for the lambda.min model when applied to the *test* data set. Be sure to show how you computed it and to display the result; once you've done that, answer the question below.

```{r}

# Your code to get the test data into the proper form to compute predicted values using the coefficients associated with the lambda.min as fitted on the training set
x_test <- model.matrix(medv ~ . - 1, data = dat.test) 
y_test <- dat.test$medv

y_pred <- predict(cvfit.house.ridge, s = lambda_min, newx = x_test)

MSPE <- mean((y_test - y_pred)^2)
MSPE

```

A) What is the mean squared prediction error you computed (your answer here): 21.13395




CONTEXT - DRINKING AND SENSATION-SEEKING

A study is described in Coxe, West, and Aiken (2009) where 400 college students were asked how many alcoholic beverages they had consumed during the previous Saturday. They also completed an eight-item subscale on the Revised NEO Personality Inventory that measures sensation-seeking.

Variables contained in this data set:

case: The participant ID
y: The number of drinks the participant reported drinking the previous Saturday
sensation: The participant's mean score on the eight-item sensation-seeking subscale
gender: The participant's gender (0 = female, 1 = male)



## Question 4 - 15 points

To date, I've given you data sets that can be read into R using base R functionality. There are numerous statistical software packages that are used out in the world, and it's possible (and, in some industries, likely) that you will encounter data sets that have a proprietary format and can only be directly opened and edited using proprietary softare. One example of this is SAS, which is celebrating it's 50th year since its original launch in 1972. It's native data format is .sas7bdat, which is similar to the .RData files you've used in that it contain metadata along with raw data. 

Do you have to have a SAS license just to open a data set a colleague sent you? Not if you know how to open it into R! 

There are two main packages for reading in non-native data formats into R. The older of the two is the foreign package, and the package that does this in the "tidyverse" constellation of packages is the haven package. For this question, you'll use the haven package to read in the drinking and sensation data set, which is in the .sas7bdat format. 

The official RStudio page for the haven package can be found here: https://haven.tidyverse.org/. Read it to find which function you will need to use to read in the SAS file. 

```{r}

drinking <- read_sas("drinking_coxe_west_aiken.sas7bdat")

str(drinking)

```

Although I expect it to come in correctly, check that the y and sensation variables are numeric before continuing. We will not be using the case or gender variables in this question.

Now you will fit three models using this data: a Poisson model, a quasipossion model, and a negative binomial model. The outcome of these analyses should be *y*, and the predictor should be *sensation*.

Poisson model
```{r}

model.poisson <- glm(y ~ sensation, data = drinking, family = poisson)

summary(model.poisson)

```

Quasipoisson model
```{r}

model.quasipoisson <- glm(y ~ sensation, data = drinking, family = quasipoisson)

summary(model.quasipoisson)

```

Negative binomial model
```{r}

model.nb <- glm.nb(y ~ sensation, data = drinking)

summary(model.nb)

```

Once you've fit all three models, answer the three questions below.

A) Look at the output for the Poisson model and the quasipoisson model. Which of the two - Poisson or quasipoisson - have *larger* standard errors for the coefficients?

Your answer here (Poisson or quasipoisson): quasipoisson

B) Look at the quasipoisson model output. What was the dispersion parameter taken to be in the quasipoisson model?

Your answer here: 2.847168

C) Per the guidelines presented in the async and discussed during the live session, which of the three models - Poisson, quasipoisson, and negative binomial - is the best based on the *residual deviance*?

Your answer here (Poisson, quasipoisson, or negative binomial): negative binomial

D) For the model that is the best based on residual deviance, which of the following statements is a correct "gist" interpretation of the coefficient associated with sensation?

Statement 1: As the mean score on the sensation-seeking subscale increases, the predicted/expected count of drinks *increases*.

Statement 2: As the mean score on the sensation-seeking subscale increases, the predicted/expected count of drinks *decreases*.

Your answer here: Statement 1 


## Question 5 - 15 points

Before beginning this question, please review the material from 9.1.3 in the async material. 

The following code is excerpted from the example shown in 9.1.3. The outcome of interest is time to death of sheep. Each sheep received some level of anti-parasite treatment; A and B contained actual anti-parasite ingredients and C was a placebo (i.e., no active ingredient in the treatment). Please run the three code chunks and examine their output. Once you've done that, answer the four questions below.

```{r}

# Chunk 1

sheep<-read.csv("sheep.deaths.csv")

with(sheep,plot(survfit(Surv(death,status)~group),lty=c(1,3,5),xlab="Age at Death (months)"))
legend("topright", c("A", "B","C"), lty = c(1,3,5))


sheep <- read.csv("sheep.deaths.csv")

censored_counts <- with(sheep, table(group, status == 0))
censored_counts


```

```{r}

# Chunk 2

model<-survreg(Surv(death,status)~group, dist="exponential",data=sheep)
summary(model)
```

```{r}

# Chunk 3

plot(survfit(Surv(sheep$death,sheep$status)~sheep$group),lty=c(1,3,5),xlab="Age at Death (months)")
legend("topright", c("A", "B","C"), lty = c(1,3,5))

points(1:50,
       1-pexp(1:50,rate=1/exp(model$coefficients[1])),
       type="l",
      lty=1)
# The survival curve S(t) for group B.
points(1:50,
       1-pexp(1:50,rate=1/exp(sum(model$coefficients[c(1,2)]))),
       type="l",
      lty=3)
# The survival curve S(t) for group C.
points(1:50,
1-pexp(1:50,rate=1/exp(sum(model$coefficients[c(1,3)]))),
       type="l",
      lty=5)

```


# Question about Chunk 1

A) What kind of plot is this? It has a specific name.

Your answer here: Kaplan-Meier survival curve

B) Which group had the most number of sheep whose outcomes were censored?

Your answer here: Group A

C) In the context of this data, what does it mean if a sheep's outcome was censored?

Your answer here: That we are not sure when exactly the sheep died (or it has been moved somewhere else)

# Questions about Chunk 2

D) What kind of survival model is being fitted in this code? Be specific. 

Your answer here: Exponential survival model

E) Looking at the p-values, is Group A significantly different from Group B?

Your answer here: Yes, Group A is significantly different from Group B (P-value is less than 0.05)

F) Looking at the p-values, is Group A significantly different from Group C?

Your answer here: Yes, Group A is significantly different from Group C (P = 2.3e-10)

G) Looking at the coefficient estimates, which group - B or C - had the lowest predicted survival time?

Your answer here: Group C had the lowest predicted survival time.

# Question about Chunk 3

H) The jagged lines on this plot are the same as those from the plot shown in Chunk 1. What is being visualized by the the *smooth, curved lines* in this plot? Again, be specific. 

Your answer here: They represent the fitted survival curves for each group (A, B, and C) based on the exponential survival model.
